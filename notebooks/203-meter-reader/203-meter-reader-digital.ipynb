{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b161d0-8144-4b3f-88fe-deb6abd21f1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Digital Meter Reader \n",
    "This notebook shows how to create a meter reader with OpenVINO Runtime. We use the pre-trained [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR) to build up a inference task pipeline:\n",
    "\n",
    "1. Configure the screen area of the meter reader.\n",
    "2. Configure the layout information of the meter reader.\n",
    "3. Preprocess the image based on the given information.\n",
    "4. Perform OCR recognition.\n",
    "5. Structure output information.\n",
    "\n",
    "This notebook will demonstrate how to read meters with different screens. In case 1 there is is a field strength meter with  general LCD screen, which means that we can read the pure text from the screen. In case 2, there is a household electricity meter with segment code LCD screen, which means we can only see the text but also the outline of the text in the background. The background outline can also be misread by the model as content that needs to be recognized, so we need to add some preprocessing module to eliminate the background information.\n",
    "\n",
    "The overall process of the two cases is similar, we only make some modifications in preprocessing for different meters. The following is the flowchart of two types of meter readers.\n",
    "\n",
    "<img align='center' src= \"https://user-images.githubusercontent.com/83450930/239390678-97ad22ad-5275-41f2-bb8a-2af83e8af0af.png\" alt=\"drawing\" width=\"1500\"/>\n",
    "\n",
    "In some cases, the screen area in the image is not in a fixed position. A detection model can be used to dynamically provide the screen area information. Please see [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) for more details.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "1. Prepare the PaddleOCR model.\n",
    "2. Define configuration and helper functions\n",
    "    - Prepare some configuration (take case 1 as an example)\n",
    "    - Define a helper function to apply an affine transformation\n",
    "    - Define helper functions for the preprocessing of text recognition\n",
    "    - Define helper functions for the postprocessing of text recognition\n",
    "3. Main Function without Segment code LCD\n",
    "    - Download Image\n",
    "    - Apply the affine transformation in the figure\n",
    "    - Define the areas to be recognition\n",
    "    - Recognize the text on the screen\n",
    "    - Postprocessing for fixing the errors in recognition\n",
    "4. Main Function with Segment code LCD\n",
    "    - Prepare the Image, some configurations, and the special preprocessing of LCD screen\n",
    "    - Recognize the text on the screen (containing the affine transformation)\n",
    "    - Postprocessing for fixing the errors in recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac00b07-32db-45c3-80f6-8243ae2c5446",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd695666-f8b8-4823-9c8f-70b7d5b14be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import copy\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386c70f-c383-48d3-a6b4-31b4ceb1653b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PaddleOCR with OpenVINO™\n",
    "\n",
    "[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) is an ultra-light OCR model trained with PaddlePaddle deep learning framework, which aims to create multilingual and practical OCR tools. \n",
    "\n",
    "The PaddleOCR pre-trained model used in the demo refers to the *\"Chinese and English ultra-lightweight PP-OCR model (9.4M)\"*. More open source pre-trained models can be downloaded at [PaddleOCR Github](https://github.com/PaddlePaddle/PaddleOCR) or [PaddleOCR Gitee](https://gitee.com/paddlepaddle/PaddleOCR).\n",
    "\n",
    "A standard PaddleOCR includes two parts of deep learning models, text detection and text recognition. This notebook only needs the text recognition part. For running the model, we first initialize the runtime for inference, then, read the network architecture and model weights from the `.pdmodel` and `.pdiparams` files to load to the CPU.\n",
    "\n",
    "More details for running PaddleOCR with OpenVINO™ are shown in [405-paddle-ocr-webcam](../405-paddle-ocr-webcam/405-paddle-ocr-webcam.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985177b2-2eac-4dc4-8c62-68e57352a172",
   "metadata": {},
   "source": [
    "### Download the Model for Text **Recognition**\n",
    "\n",
    "The pre-trained models used in the demo are downloaded and stored in the \"model\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278be3b7-a86c-44e1-8a87-1b9c7ec67613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"model\"\n",
    "RECOGNITION_MODEL_LINK = \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\"\n",
    "RECOGNITION_FILE_NAME = RECOGNITION_MODEL_LINK.split(\"/\")[-1]\n",
    "\n",
    "# download file\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "utils.download_file(RECOGNITION_MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "file = tarfile.open(f\"model/{RECOGNITION_FILE_NAME}\")\n",
    "res = file.extractall(\"model\")\n",
    "if not res:\n",
    "    print(f\"Detection Model Extracted to \\\"./{MODEL_DIR}\\\".\")\n",
    "else:\n",
    "    print(\"Error Extracting the Detection model. Please check the network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2135e6-3a41-4459-ae8e-0591ed58ebb6",
   "metadata": {},
   "source": [
    "### Load the Model for Text **Recognition** with Dynamic Shape\n",
    "\n",
    "Input to text recognition model refers to detected bounding boxes with different image sizes, for example, dynamic input shapes. Hence:\n",
    "\n",
    "1. Input dimension with dynamic input shapes needs to be specified before loading the text recognition model.\n",
    "2. Dynamic shape is specified by assigning -1 to the input dimension or by setting the upper bound of the input dimension using, for example, `Dimension(1, 512)`.\n",
    "\n",
    ">Note: Since the text recognition model is with dynamic input shape and the current release of OpenVINO 2022.2 does not support dynamic shape on iGPU, you cannot directly switch the device to iGPU for inference in this case. Otherwise, you may need to resize the input images to this model into a fixed size and then try running the inference on iGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eef6db-ae79-43fc-aade-382fd07c5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime for text recognition.\n",
    "core = Core()\n",
    "\n",
    "# Read the model and corresponding weights from a file.\n",
    "rec_model_file_path = f\"{MODEL_DIR}/ch_PP-OCRv3_rec_infer/inference.pdmodel\"\n",
    "rec_model = core.read_model(model=rec_model_file_path)\n",
    "\n",
    "# Assign dynamic shapes to every input layer on the last dimension.\n",
    "for input_layer in rec_model.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[3] = -1\n",
    "    rec_model.reshape({input_layer: input_shape})\n",
    "\n",
    "rec_compiled_model = core.compile_model(model=rec_model, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output nodes.\n",
    "rec_input_layer = rec_compiled_model.input(0)\n",
    "rec_output_layer = rec_compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2f20-103b-42dc-b2c5-ca71e9f5dd1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration and Helper functions\n",
    "\n",
    "To structure output, we should first config some parameters (for example, the coordinates of the corners of the screen).\n",
    "\n",
    "Then, use the following helper functions for preprocessing and postprocessing frames:\n",
    "\n",
    "1. Preprocessing the input image: use affine transformations to normalize skewed images.\n",
    "2. Preprocessing for text recognition: resize and normalize detected box images to the same size (for example, `(3, 32, 320)` size for images with Chinese text) for easy batching in inference.\n",
    "3. Postprocessing for structure output: fix some errors in recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006146a-07c4-4b9d-9f3d-cb2f97a71aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "There are four pieces of information to config:\n",
    "1. POINTS: the edge of the meter's screen, which can be determined by four corners\n",
    "2. DESIGN_SHAPE: the original shape of the screen\n",
    "3. RESULT_TEMP: a template of output, which is a directory. The keys in the `dict` are output features.\n",
    "4. DESIGN_LAYOUT: the elements and their positions in the screen, which is in a standard DESIGN_SHAPE. Commonly, after the image is converted into a regular image, a frame can be drawn directly on the image to delineate the position of each element. So we define the DESIGN_LAYOUT `dict` after `Preprocessing the input image`. \n",
    "\n",
    "> note: The concepts of 'DESIGN_SHAPE' and 'DESIGN_LAYOUT' refer to the fact that the layout information of a display screen is determined during design, and these information is independent of the image. Therefore, you can obtain 'DESIGN_LAYOUT' information from your device manual.\n",
    "However, a simple way to run the program is to regard the coordinates of some points in figure as 'DESIGN_LAYOUT'. You can use `Paint`, a windows application, to abtain the coordinates. You only need to move the mouse to a position on the image, and the tool will display the corresponding coordinates.\n",
    "\n",
    "In the example image below, there are 8 features to output.\n",
    "1. Info_Probe: Probe information of power frequency field strength meter. \"探头:---\" means there is no information about Probe.\n",
    "2. Freq_Set: The work frequency, and the \"实时值\" is a const text to tell user that the preceding numbers are real-time.\n",
    "3. Val_Total: The value measured. \"无探头\" means there is no information about Porbe, otherwise, there should be a float value.\n",
    "4. Val_X: Value from the x-axis.\n",
    "5. Val_Y: Value from the y-axis.\n",
    "6. Val_Z: Value from the z-axis.\n",
    "7. Unit\n",
    "8. Field: One of the Conventional, electric field, magnetic field. The Chinese word \"电场\" means electric field.\n",
    "\n",
    "<img align='center' src= \"https://user-images.githubusercontent.com/83450930/236680146-5751e291-d509-4d71-a2cb-bfbf35609051.jpg\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3cb3af-2ff2-4b26-96d8-71560ae6ea93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The coordinates of the corners of the screen in case 1\n",
    "POINTS = [[1121, 56],    # Left top\n",
    "          [3242, 183],   # right top\n",
    "          [3040, 1841],  # right bottom\n",
    "          [1000, 1543]]   # left bottom\n",
    "\n",
    "# The size of the screen in case 1\n",
    "DESIGN_SHAPE = (1300, 1000)\n",
    "\n",
    "# Output template in case 1\n",
    "RESULT_TEMP = {\"Info_Probe\":\"探头:---\", \n",
    "               \"Freq_Set\":\"\", \n",
    "               \"Val_Total\":\"无探头\", \n",
    "               \"Val_X\":\"\", \n",
    "               \"Val_Y\":\"\", \n",
    "               \"Val_Z\":\"\", \n",
    "               \"Unit\":\"A/m\", \n",
    "               \"Field\":\"常规\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fc249-ba1c-40b9-889b-4bfd04cc7576",
   "metadata": {},
   "source": [
    "### Preprocessing the input image\n",
    "\n",
    "Use affine transformations to normalize skewed images. After the preprocessing, the DESIGN_LAYOUT can be given manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c381bcb-04ba-49b0-b8ec-1cf90b5d4e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_processing(img, point_list, target_shape):\n",
    "    \"\"\"\n",
    "    Preprocessing function for normalizing skewed images.\n",
    "    Parameters:\n",
    "        img (np.ndarray): Input image.\n",
    "        point_list (List[List[int, int], List[int, int], List[int, int]]): Coordinates of the corners of the screen.\n",
    "        target_shape (Tuple(int, int)): The design shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    # affine transformations\n",
    "    # point list is the coordinates of the corners of the screen\n",
    "    # target shape is the design shape\n",
    "    \n",
    "    target_w, target_h = target_shape\n",
    "    pts1 = np.float32(point_list)\n",
    "    pts2 = np.float32([[0, 0],[target_w,0],[target_w, target_h],[0,target_h]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    img2 = cv2.warpPerspective(img, M, (target_w,target_h))\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a7459-5fc8-4254-a32e-a88f701493c6",
   "metadata": {},
   "source": [
    "### Preprocessing Image Functions for Text Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fbc3d-0adc-4360-b1fa-10248ccc4128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess for text recognition.\n",
    "def resize_norm_img(img, max_wh_ratio):\n",
    "    \"\"\"\n",
    "    Resize input image for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img: bounding box image from text detection \n",
    "        max_wh_ratio: value for the resizing for text recognition model\n",
    "    \"\"\"\n",
    "    rec_image_shape = [3, 48, 320]\n",
    "    imgC, imgH, imgW = rec_image_shape\n",
    "    assert imgC == img.shape[2]\n",
    "    character_type = \"ch\"\n",
    "    if character_type == \"ch\":\n",
    "        imgW = int((32 * max_wh_ratio))\n",
    "    h, w = img.shape[:2]\n",
    "    ratio = w / float(h)\n",
    "    if math.ceil(imgH * ratio) > imgW:\n",
    "        resized_w = imgW\n",
    "    else:\n",
    "        resized_w = int(math.ceil(imgH * ratio))\n",
    "    resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "    resized_image = resized_image.astype('float32')\n",
    "    resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "    resized_image -= 0.5\n",
    "    resized_image /= 0.5\n",
    "    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "    padding_im[:, :, 0:resized_w] = resized_image\n",
    "    return padding_im\n",
    "\n",
    "\n",
    "def get_rotate_crop_image(img, points):\n",
    "    '''\n",
    "    img_height, img_width = img.shape[0:2]\n",
    "    left = int(np.min(points[:, 0]))\n",
    "    right = int(np.max(points[:, 0]))\n",
    "    top = int(np.min(points[:, 1]))\n",
    "    bottom = int(np.max(points[:, 1]))\n",
    "    img_crop = img[top:bottom, left:right, :].copy()\n",
    "    points[:, 0] = points[:, 0] - left\n",
    "    points[:, 1] = points[:, 1] - top\n",
    "    '''\n",
    "    assert len(points) == 4, \"shape of points must be 4*2\"\n",
    "    img_crop_width = int(\n",
    "        max(\n",
    "            np.linalg.norm(points[0] - points[1]),\n",
    "            np.linalg.norm(points[2] - points[3])))\n",
    "    img_crop_height = int(\n",
    "        max(\n",
    "            np.linalg.norm(points[0] - points[3]),\n",
    "            np.linalg.norm(points[1] - points[2])))\n",
    "    pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
    "                          [img_crop_width, img_crop_height],\n",
    "                          [0, img_crop_height]])\n",
    "    M = cv2.getPerspectiveTransform(points, pts_std)\n",
    "    dst_img = cv2.warpPerspective(\n",
    "        img,\n",
    "        M, (img_crop_width, img_crop_height),\n",
    "        borderMode=cv2.BORDER_REPLICATE,\n",
    "        flags=cv2.INTER_CUBIC)\n",
    "    dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
    "    if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
    "        dst_img = np.rot90(dst_img)\n",
    "    return dst_img\n",
    "\n",
    "\n",
    "def prep_for_rec(dt_boxes, frame):\n",
    "    \"\"\"\n",
    "    Preprocessing of the detected bounding boxes for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        dt_boxes: detected bounding boxes from text detection \n",
    "        frame: original input frame \n",
    "    \"\"\"\n",
    "    ori_im = frame.copy()\n",
    "    img_crop_list = [] \n",
    "    for bno in range(len(dt_boxes)):\n",
    "        tmp_box = copy.deepcopy(dt_boxes[bno])\n",
    "        img_crop = get_rotate_crop_image(ori_im, tmp_box)\n",
    "        img_crop_list.append(img_crop)\n",
    "        \n",
    "    img_num = len(img_crop_list)\n",
    "    # Calculate the aspect ratio of all text bars.\n",
    "    width_list = []\n",
    "    for img in img_crop_list:\n",
    "        width_list.append(img.shape[1] / float(img.shape[0]))\n",
    "    \n",
    "    # Sorting can speed up the recognition process.\n",
    "    indices = np.argsort(np.array(width_list))\n",
    "    return img_crop_list, img_num, indices\n",
    "\n",
    "\n",
    "def batch_text_box(img_crop_list, img_num, indices, beg_img_no, batch_num):\n",
    "    \"\"\"\n",
    "    Batch for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img_crop_list: processed detected bounding box images \n",
    "        img_num: number of bounding boxes from text detection\n",
    "        indices: sorting for bounding boxes to speed up text recognition\n",
    "        beg_img_no: the beginning number of bounding boxes for each batch of text recognition inference\n",
    "        batch_num: number of images for each batch\n",
    "    \"\"\"\n",
    "    norm_img_batch = []\n",
    "    max_wh_ratio = 0\n",
    "    end_img_no = min(img_num, beg_img_no + batch_num)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        h, w = img_crop_list[indices[ino]].shape[0:2]\n",
    "        wh_ratio = w * 1.0 / h\n",
    "        max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        norm_img = resize_norm_img(img_crop_list[indices[ino]], max_wh_ratio)\n",
    "        norm_img = norm_img[np.newaxis, :]\n",
    "        norm_img_batch.append(norm_img)\n",
    "\n",
    "    norm_img_batch = np.concatenate(norm_img_batch)\n",
    "    norm_img_batch = norm_img_batch.copy()\n",
    "    return norm_img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacace0-c6a1-4678-b5b9-263d2e4c0ee7",
   "metadata": {},
   "source": [
    "### Postprocessing Image Functions for Text Recognition\n",
    "\n",
    "The results of text recognition are text-index, we should convert is to the characters. The following codes construct a decoder for the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca24a6f-1cb2-4a44-9bc9-305eda51c466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecLabelDecode(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 character_dict_path=None,\n",
    "                 character_type='ch',\n",
    "                 use_space_char=False):\n",
    "        support_character_type = [\n",
    "            'ch', 'en', 'EN_symbol', 'french', 'german', 'japan', 'korean',\n",
    "            'it', 'xi', 'pu', 'ru', 'ar', 'ta', 'ug', 'fa', 'ur', 'rs', 'oc',\n",
    "            'rsc', 'bg', 'uk', 'be', 'te', 'ka', 'chinese_cht', 'hi', 'mr',\n",
    "            'ne', 'EN', 'latin', 'arabic', 'cyrillic', 'devanagari'\n",
    "        ]\n",
    "        assert character_type in support_character_type, \"Only {} are supported now but get {}\".format(\n",
    "            support_character_type, character_type)\n",
    "\n",
    "        self.beg_str = \"sos\"\n",
    "        self.end_str = \"eos\"\n",
    "\n",
    "        if character_type == \"en\":\n",
    "            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "            dict_character = list(self.character_str)\n",
    "        elif character_type == \"EN_symbol\":\n",
    "            # same with ASTER setting (use 94 char).\n",
    "            self.character_str = string.printable[:-6]\n",
    "            dict_character = list(self.character_str)\n",
    "        elif character_type in support_character_type:\n",
    "            self.character_str = []\n",
    "            assert character_dict_path is not None, \"character_dict_path should not be None when character_type is {}\".format(\n",
    "                character_type)\n",
    "            with open(character_dict_path, \"rb\") as fin:\n",
    "                lines = fin.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.decode('utf-8').strip(\"\\n\").strip(\"\\r\\n\")\n",
    "                    self.character_str.append(line)\n",
    "            if use_space_char:\n",
    "                self.character_str.append(\" \")\n",
    "            dict_character = list(self.character_str)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.character_type = character_type\n",
    "        dict_character = self.add_special_char(dict_character)\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(dict_character):\n",
    "            self.dict[char] = i\n",
    "        self.character = dict_character\n",
    "\n",
    "        \n",
    "    def __call__(self, preds, label=None, *args, **kwargs):\n",
    "        preds_idx = preds.argmax(axis=2)\n",
    "        preds_prob = preds.max(axis=2)\n",
    "        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=True)\n",
    "        if label is None:\n",
    "            return text\n",
    "        label = self.decode(label)\n",
    "        return text, label\n",
    "\n",
    "    \n",
    "    def add_special_char(self, dict_character):\n",
    "        dict_character = ['blank'] + dict_character\n",
    "        return dict_character\n",
    "\n",
    "    \n",
    "    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        result_list = []\n",
    "        ignored_tokens = self.get_ignored_tokens()\n",
    "        batch_size = len(text_index)\n",
    "        for batch_idx in range(batch_size):\n",
    "            char_list = []\n",
    "            conf_list = []\n",
    "            for idx in range(len(text_index[batch_idx])):\n",
    "                if text_index[batch_idx][idx] in ignored_tokens:\n",
    "                    continue\n",
    "                if is_remove_duplicate:\n",
    "                    # only for predict\n",
    "                    if idx > 0 and text_index[batch_idx][idx - 1] == text_index[\n",
    "                            batch_idx][idx]:\n",
    "                        continue\n",
    "                char_list.append(self.character[int(text_index[batch_idx][\n",
    "                    idx])])\n",
    "                if text_prob is not None:\n",
    "                    conf_list.append(text_prob[batch_idx][idx])\n",
    "                else:\n",
    "                    conf_list.append(1)\n",
    "            text = ''.join(char_list)\n",
    "            result_list.append((text, np.mean(conf_list)))\n",
    "        return result_list\n",
    "\n",
    "    \n",
    "    def get_ignored_tokens(self):\n",
    "        return [0]  # for ctc blank\n",
    "\n",
    "\n",
    "# Since the recognition results contain chinese words, we should use 'ch' as character_type\n",
    "text_decoder = RecLabelDecode(character_dict_path=\"../data/text/ppocr_keys_v1.txt\",\n",
    "                              character_type='ch',  \n",
    "                              use_space_char=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560200a-1392-434b-b11e-07aa7d14aecf",
   "metadata": {},
   "source": [
    "### Postprocessing the output information\n",
    "The results of text recognition may contain some errors or unimportant information. In order to make the recognition results more accurate, we establish an auxiliary function. This auxiliary function corrects the text recognition results for the following three situations:\n",
    "\n",
    "1. Information that needs to be removed or replacing (keyword: 'RP'): For example, when recognizing values of Val_X, 'X:' will be recognized together, where 'X:' can be replace by '' from the final recognition result.\n",
    "2. Information to be mapped (keyword: 'MP'): 'k/m' may be recognized as km, so km can be mapped as' k/m'.\n",
    "3. Decimal point addition (keyword 'AD'): Sometimes decimal points may be missed. If the number of decimal places is known in advance, decimal points can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc138ac-73f5-46d4-abb6-a67dcf6e9ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post-processing, fix some error made in recognition\n",
    "def post_processing(results, post_configration):\n",
    "    \"\"\"\n",
    "    Postprocessing function for correcting the recognition errors.\n",
    "    Parameters:\n",
    "        results (Dict): The result directory.\n",
    "        post_configration (Dict): The configuration directory.\n",
    "    \"\"\"\n",
    "    for key in post_configration.keys():\n",
    "        if len(post_configration[key]) == 0:\n",
    "            continue  # nothing to do\n",
    "        for post_item in post_configration[key]:\n",
    "            key_word = post_item[0]\n",
    "            if key_word == 'MP':  # mapping\n",
    "                source_word = post_item[1]\n",
    "                target_word = post_item[2]\n",
    "                if source_word in results[key]:\n",
    "                    results[key] = target_word\n",
    "            elif key_word == 'RP':  # removing\n",
    "                source_word = post_item[1]\n",
    "                target_word = post_item[2]\n",
    "                results[key] = results[key].replace(source_word, target_word)\n",
    "            elif key_word == 'AD':  # add point\n",
    "                add_position = post_item[1]\n",
    "                results[key] = results[key][:add_position] + '.' + results[key][add_position:]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545be5e-a358-442a-8640-08d139f752c5",
   "metadata": {},
   "source": [
    "After estabilishing the function, only a configuration dictionary needs to be passed in for post-processing, such as `['MP', 'LF', '探头:LF-01']`, `['RP', 'X', ':']`, `['AD', -2]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2daec-d321-4249-bb17-a52f97cb7577",
   "metadata": {},
   "source": [
    "## Main Function without Segment code LCD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d59075-288a-46d0-b049-ed656fa7cf55",
   "metadata": {},
   "source": [
    "### Get Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb78ff-b376-4777-bce3-b995c079f13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download images\n",
    "IMG_URL = \"https://user-images.githubusercontent.com/83450930/236680146-5751e291-d509-4d71-a2cb-bfbf35609051.jpg\"\n",
    "IMG_FILE_NAME = IMG_URL.split(\"/\")[-1]\n",
    "utils.download_file(IMG_URL, show_progress=False)\n",
    "\n",
    "# Read image\n",
    "img = cv2.imread(IMG_FILE_NAME)\n",
    "\n",
    "# Show input image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0098067-45d3-42fe-b77f-4a4b62a34d9a",
   "metadata": {},
   "source": [
    "### Preprocessing the input image\n",
    "\n",
    "Cut the screen part and use affine transformations to normalize skewed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b15cf-d519-4aa7-8dca-3e36e337f17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# affine transformations to normalize skewed images\n",
    "img = pre_processing(img, POINTS, DESIGN_SHAPE)\n",
    "\n",
    "# The screen part is cut and corrected\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65d723-2b73-4d6f-9a02-f45c6e1f22e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selecte the Region to be recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82571a0d-4779-41a3-b547-803da14b9b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features and the layout information\n",
    "DESIGN_LAYOUT = {'Info_Probe':[14, 36, 410, 135],  # feature_name, xmin, ymin, xmax, ymax\n",
    "                 'Freq_Set':[5, 290, 544, 406], \n",
    "                 'Val_Total':[52, 419, 1256, 741], \n",
    "                 'Val_X':[19, 774, 433, 882], \n",
    "                 'Val_Y':[433, 773, 874, 884], \n",
    "                 'Val_Z':[873, 773, 1276, 883], \n",
    "                 'Unit':[1064, 291, 1295, 403], \n",
    "                 'Field':[5, 913, 243, 998]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0eabc-d187-4d84-963d-cda676daff89",
   "metadata": {},
   "source": [
    "### Cut the element fields and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504e2d5-7fd0-47e1-8500-10f67ebcf96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the input for recognition need the image, DESIGN information, compiled_model\n",
    "def main_for_field_strength_meter(img, DESIGN_LAYOUT, RESULT_TEMP, rec_compiled_model, rec_output_layer, text_decoder):\n",
    "    \"\"\"\n",
    "    Main program of processing the field strength meter.\n",
    "    Parameters:\n",
    "        img (np.ndarray): Input image.\n",
    "        DESIGN_LAYOUT (Dict): The coordinates of elements in the screen.\n",
    "        RESULT_TEMP (Dict): The template for structure output.\n",
    "        rec_compiled_model: CompiledModel.\n",
    "        rec_output_layer: The output of openvino model.\n",
    "        text_decoder(RecLabelDecode): The decoder of raw-recognition results.\n",
    "    \"\"\"\n",
    "    # copy the structure output template\n",
    "    struct_result = copy.deepcopy(RESULT_TEMP)\n",
    "\n",
    "    # structure recognition begins here\n",
    "    for key in DESIGN_LAYOUT.keys():\n",
    "        # cut imgs according the layout information\n",
    "        xmin, ymin, xmax, ymax = DESIGN_LAYOUT[key]\n",
    "        cut_img = img[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        h = ymax - ymin  # height of cut_img\n",
    "        w = xmax - xmin  # width of cut_img\n",
    "        dt_boxes = [np.array([[0,0],[w,0],[w,h],[0,h]],dtype='float32')]\n",
    "        batch_num = 1\n",
    "\n",
    "        # since the input img is cut, we do not need a detection model to find the position of texts\n",
    "        # Preprocess detection results for recognition.\n",
    "        img_crop_list, img_num, indices = prep_for_rec(dt_boxes, cut_img)\n",
    "\n",
    "        # txts are the recognized text results\n",
    "        rec_res = [['', 0.0]] * img_num\n",
    "        txts = [] \n",
    "\n",
    "        for beg_img_no in range(0, img_num):\n",
    "\n",
    "            # Recognition starts from here.\n",
    "            norm_img_batch = batch_text_box(\n",
    "                img_crop_list, img_num, indices, beg_img_no, batch_num)\n",
    "\n",
    "            # Run inference for text recognition. \n",
    "            rec_results = rec_compiled_model([norm_img_batch])[rec_output_layer]\n",
    "\n",
    "            # Postprocessing recognition results.\n",
    "            rec_result = text_decoder(rec_results)\n",
    "            for rno in range(len(rec_result)):\n",
    "                rec_res[indices[beg_img_no + rno]] = rec_result[rno]   \n",
    "            if rec_res:\n",
    "                txts = [rec_res[i][0] for i in range(len(rec_res))] \n",
    "\n",
    "        # record the recognition result\n",
    "        struct_result[key] = txts[0]\n",
    "        \n",
    "    return struct_result\n",
    "\n",
    "struct_result = main_for_field_strength_meter(img, DESIGN_LAYOUT, RESULT_TEMP, rec_compiled_model, rec_output_layer, text_decoder)\n",
    "        \n",
    "# the raw output information\n",
    "print(struct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afe33a-9218-4c1a-941e-931e087265e0",
   "metadata": {},
   "source": [
    "### Postprocessing the output information\n",
    "For correcting the error in recognition, we only need to configure a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc916df8-d486-47f5-8318-4eb2e32fb2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Congiguration for postprocessing of the results\n",
    "RESULT_POST = {\"Info_Probe\":[['MP', 'LF', '探头:LF-01']],  # words need to be mapped\n",
    "               \"Freq_Set\":[['RP', '实时值', ''], ['RP', ' ', '']],  # words need to be replace\n",
    "               \"Val_Total\":[['RP', 'H2', 'Hz']],\n",
    "               \"Val_X\":[['RP', 'X', ''], ['RP', ':', '']], \n",
    "               \"Val_Y\":[['RP', 'Y', ''], ['RP', ':', '']], \n",
    "               \"Val_Z\":[['RP', 'Z', ''], ['RP', ':', '']], \n",
    "               \"Unit\":[['MP', 'T', 'μT'],['MP', 'kV', 'kV/m'],['MP', 'kv', 'kV/m'],['MP', 'vm', 'V/m'],['MP', 'Vm', 'V/m'],['MP', 'A', 'A/m']], \n",
    "               \"Field\":[]}  # nothing need to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca419b-3fe8-47ef-9151-c32335729499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing, to fix some error made in recognition\n",
    "struct_result = post_processing(struct_result, RESULT_POST)\n",
    "\n",
    "# Print result\n",
    "print(struct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a7b45-cfc7-400e-897b-5fdff2c78042",
   "metadata": {},
   "source": [
    "## Main Function for segment code LCD screen\n",
    "\n",
    "Here shows how to extract the value in the screen of domestic intelligent meter.\n",
    "\n",
    "<img align='center' src= \"https://user-images.githubusercontent.com/83450930/237240032-fa150a14-800c-4525-ad4f-686165d23aa4.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "\n",
    "This screen is Segment code LCD, where we can see the outline of the text in the background, but only the highlighted parts need to be recognized. In this case, the model may also consider the background content as a part that needs to be recognized. The information displayed on this screen is not centered, which is also detrimental to the text recognition model.\n",
    "\n",
    "The following steps can be taken to address the above issues:\n",
    "\n",
    "1. Perform binary processing on the image. By specifying a threshold, background information can be easily and quickly removed.\n",
    "2. Separate the digital display area into many small blocks and remove those solid colored blocks to ensure that the text in final image for recognition is centered.\n",
    "\n",
    "In addition, we can also add an external detection model to center the recognized text, or train a better recognition model to improve recognition accuracy. But these strategies all come with higher computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9798e-a804-4af9-a64f-8db75d0033ab",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc106c-0209-44f6-b0c8-e9b2d3b2fc93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the image, configuration and pre/post-processing\n",
    "\n",
    "# Download images\n",
    "IMG_URL = \"https://user-images.githubusercontent.com/83450930/237240032-fa150a14-800c-4525-ad4f-686165d23aa4.png\"\n",
    "IMG_FILE_NAME = IMG_URL.split(\"/\")[-1]\n",
    "utils.download_file(IMG_URL, show_progress=False)\n",
    "\n",
    "# The coordinates of the corners of the screen\n",
    "POINTS = [[275, 676],    # Left top, [x, y]\n",
    "          [1775, 704],   # right top\n",
    "          [1829, 2439],  # right bottom\n",
    "          [205, 2469]]   # left bottom\n",
    "\n",
    "# The size of the screen\n",
    "DESIGN_SHAPE = (1500, 1800)\n",
    "\n",
    "# Output template\n",
    "RESULT_TEMP = {\"Value\":\"\"}\n",
    "\n",
    "# features and the layout information\n",
    "DESIGN_LAYOUT = {'Value':[470, 436, 1050, 592]}\n",
    "\n",
    "# a special pre-processing for the LCD screen\n",
    "def pre_processing_for_LCD(cut_img):\n",
    "    \"\"\"\n",
    "    A process function for the Segment code LCD\n",
    "    Parameters:\n",
    "        cut_img (np.ndarray): Input image.\n",
    "    \"\"\"\n",
    "    # BGR-image to BINARY-image\n",
    "    cut_img = cv2.cvtColor(cut_img, cv2.COLOR_BGR2GRAY)\n",
    "    _, cut_img = cv2.threshold(cut_img, 64, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # delete the area without text\n",
    "    step_size = 30\n",
    "    for i in range(0, cut_img.shape[1], step_size):\n",
    "        rate = len(np.where(cut_img[:, i:i + step_size] > 128)[0]) / (cut_img[:, i:i + step_size].shape[0] * cut_img[:, i:i + step_size].shape[1])\n",
    "        if rate <= 0.95:\n",
    "            cut_img = cut_img[:, i:]\n",
    "            break\n",
    "    \n",
    "    # [h, w] to [h, w, 3]\n",
    "    cut_img = np.expand_dims(cut_img, axis=2)\n",
    "    cut_img = np.concatenate((cut_img, cut_img, cut_img), axis=-1)\n",
    "    \n",
    "    return cut_img\n",
    "    \n",
    "# Configuration for postprocessing of the results\n",
    "RESULT_POST = {\"Value\":[['RP', ' ', ''], ['AD', -2]]}  # add point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9789d1-465f-41b9-b1d4-8e52cb81f906",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d164fa-59ec-4aa8-ba2f-8e270e762ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read image\n",
    "img = cv2.imread(IMG_FILE_NAME)\n",
    "\n",
    "# pre-processing\n",
    "img = pre_processing(img, POINTS, DESIGN_SHAPE)\n",
    "\n",
    "# Show pre-processed image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a958dfe-2aa7-4d64-a974-21522e0d86dc",
   "metadata": {},
   "source": [
    "### Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b43f6b-fb1b-4cb4-a9cd-a6f7bf1812fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the input for recognition need the image, DESIGN information, compiled_model\n",
    "def main_for_electricity_meter(img, DESIGN_LAYOUT, RESULT_TEMP, rec_compiled_model, rec_output_layer, text_decoder):\n",
    "    \"\"\"\n",
    "    Main program of processing the electricity meter.\n",
    "    Parameters:\n",
    "        img (np.ndarray): Input image.\n",
    "        DESIGN_LAYOUT (Dict): The coordinates of elements in the screen.\n",
    "        RESULT_TEMP (Dict): The template for structure output.\n",
    "        rec_compiled_model: CompiledModel.\n",
    "        rec_output_layer: The output of openvino model.\n",
    "        text_decoder(RecLabelDecode): The decoder of raw-recognition results.\n",
    "    \"\"\"\n",
    "    # copy the structure output template\n",
    "    struct_result = copy.deepcopy(RESULT_TEMP)\n",
    "\n",
    "    # structure recognition begins here\n",
    "    for key in DESIGN_LAYOUT.keys():\n",
    "        # cut imgs according the layout information\n",
    "        xmin, ymin, xmax, ymax = DESIGN_LAYOUT[key]\n",
    "        cut_img = img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        if key == 'Value':\n",
    "            # show the image before LCD pre-processing\n",
    "            print('Left is the Value Part before the pre_processing_for_LCD')\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(cut_img)\n",
    "\n",
    "            cut_img = pre_processing_for_LCD(cut_img)\n",
    "\n",
    "            # show the image after LCD pre-processing\n",
    "            print('Right is the Value Part after the pre_processing_for_LCD')\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(cut_img)\n",
    "\n",
    "        h = ymax - ymin  # height of cut_img\n",
    "        w = xmax - xmin  # width of cut_img\n",
    "        dt_boxes = [np.array([[0,0],[w,0],[w,h],[0,h]],dtype='float32')]\n",
    "        batch_num = 1\n",
    "\n",
    "        # since the input img is cut, we do not need a detection model to find the position of texts\n",
    "        # Preprocess detection results for recognition.\n",
    "        img_crop_list, img_num, indices = prep_for_rec(dt_boxes, cut_img)\n",
    "\n",
    "        # txts are the recognized text results\n",
    "        rec_res = [['', 0.0]] * img_num\n",
    "        txts = [] \n",
    "\n",
    "        for beg_img_no in range(0, img_num):\n",
    "\n",
    "            # Recognition starts from here.\n",
    "            norm_img_batch = batch_text_box(\n",
    "                img_crop_list, img_num, indices, beg_img_no, batch_num)\n",
    "\n",
    "            # Run inference for text recognition. \n",
    "            rec_results = rec_compiled_model([norm_img_batch])[rec_output_layer]\n",
    "\n",
    "            # Postprocessing recognition results.\n",
    "            rec_result = text_decoder(rec_results)\n",
    "            for rno in range(len(rec_result)):\n",
    "                rec_res[indices[beg_img_no + rno]] = rec_result[rno]   \n",
    "            if rec_res:\n",
    "                txts = [rec_res[i][0] for i in range(len(rec_res))] \n",
    "\n",
    "        # record the recognition result\n",
    "        struct_result[key] = txts[0]\n",
    "        \n",
    "        return struct_result\n",
    "\n",
    "struct_result = main_for_electricity_meter(img, DESIGN_LAYOUT, RESULT_TEMP, rec_compiled_model, rec_output_layer, text_decoder)\n",
    "        \n",
    "# the raw output information\n",
    "print(struct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14937990-45da-4e99-b5c1-3740a34af633",
   "metadata": {},
   "source": [
    "### Post-processing of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32e6c7-c5e4-475d-bf4c-3a281532d006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post-processing, fix some error made in recognition\n",
    "struct_result = post_processing(struct_result, RESULT_POST)\n",
    "\n",
    "# the final output information\n",
    "print(struct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb52b8-0a6c-4b05-b7ca-d341ffa88bad",
   "metadata": {},
   "source": [
    " ## Try it with your meter photos!\n",
    " \n",
    " For your photos, you only need to modify the `Configuration` and `post_processing` to run above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
