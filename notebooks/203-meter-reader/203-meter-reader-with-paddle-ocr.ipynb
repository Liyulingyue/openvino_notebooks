{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b161d0-8144-4b3f-88fe-deb6abd21f1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Meter Reader with PaddleOCR\n",
    "This notebook shows how to create a meter reader with OpenVINO Runtime. We use the pre-trained [PP-OCR](https://github.com/PaddlePaddle/PaddleOCR) to build up a inference task pipeline:\n",
    "\n",
    "1. Config the screen area of the meter reader.\n",
    "2. Config the layout information of the meter reader.\n",
    "3. Pre-process the image based on the given information.\n",
    "4. Perform OCR recognition.\n",
    "5. Structure output information.\n",
    "\n",
    "<img align='center' src= \"https://user-images.githubusercontent.com/83450930/236680983-f23e8728-c7f9-4460-8794-44fac360a4ac.png\" alt=\"drawing\" width=\"1500\"/>\n",
    "\n",
    "In some cases, the screen area in the image is not in a fixed position. A detection model can be used to dynamically provide the screen area information. Please see [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac00b07-32db-45c3-80f6-8243ae2c5446",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ccbd7-a08b-4d47-b1ff-f8b0f9886295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q \"pyclipper>=1.2.1\" \"shapely>=1.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd695666-f8b8-4823-9c8f-70b7d5b14be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "import processing as processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386c70f-c383-48d3-a6b4-31b4ceb1653b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PaddleOCR with OpenVINO™\n",
    "\n",
    "[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) is an ultra-light OCR model trained with PaddlePaddle deep learning framework, which aims to create multilingual and practical OCR tools. \n",
    "\n",
    "The PaddleOCR pre-trained model used in the demo refers to the *\"Chinese and English ultra-lightweight PP-OCR model (9.4M)\"*. More open source pre-trained models can be downloaded at [PaddleOCR Github](https://github.com/PaddlePaddle/PaddleOCR) or [PaddleOCR Gitee](https://gitee.com/paddlepaddle/PaddleOCR).\n",
    "\n",
    "A standard PaddleOCR includes two parts of deep learning models, text detection and text recognition. This notebook only needs the text recognition part. For running the model, we first initialize the runtime for inference, then, read the network architecture and model weights from the `.pdmodel` and `.pdiparams` files to load to CPU.\n",
    "\n",
    "More details for running PaddleOCR with OpenVINO™ are shown in [405-paddle-ocr-webcam](../405-paddle-ocr-webcam/405-paddle-ocr-webcam.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985177b2-2eac-4dc4-8c62-68e57352a172",
   "metadata": {},
   "source": [
    "### Download the Model for Text **Recognition**\n",
    "\n",
    "The pre-trained models used in the demo are downloaded and stored in the \"model\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec981e3-6d52-4de9-82dc-10506574abfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to download text detection and recognition models from PaddleOCR resources.\n",
    "\n",
    "def run_model_download(model_url, model_file_path):\n",
    "    \"\"\"\n",
    "    Download pre-trained models from PaddleOCR resources\n",
    "\n",
    "    Parameters:\n",
    "        model_url: url link to pre-trained models\n",
    "        model_file_path: file path to store the downloaded model\n",
    "    \"\"\"\n",
    "    model_name = model_url.split(\"/\")[-1]\n",
    "    \n",
    "    if model_file_path.is_file(): \n",
    "        print(\"Model already exists\")\n",
    "    else:\n",
    "        # Download the model from the server, and untar it.\n",
    "        print(\"Downloading the pre-trained model... May take a while...\")\n",
    "\n",
    "        # Create a directory.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        response = requests.get(model_url)\n",
    "        with open(f\"model/{model_name}\", \"wb\") as model_tar_file:\n",
    "            model_tar_file.write(response.content)\n",
    "        print(\"Model Downloaded\")\n",
    "\n",
    "        file = tarfile.open(f\"model/{model_name}\")\n",
    "        res = file.extractall(\"model\")\n",
    "        file.close()\n",
    "        if not res:\n",
    "            print(f\"Model Extracted to {model_file_path}.\")\n",
    "        else:\n",
    "            print(\"Error Extracting the model. Please check the network.\")\n",
    "            \n",
    "rec_model_url = \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\"\n",
    "rec_model_file_path = Path(\"model/ch_PP-OCRv3_rec_infer/inference.pdmodel\")\n",
    "\n",
    "run_model_download(rec_model_url, rec_model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2135e6-3a41-4459-ae8e-0591ed58ebb6",
   "metadata": {},
   "source": [
    "### Load the Model for Text **Recognition** with Dynamic Shape\n",
    "\n",
    "Input to text recognition model refers to detected bounding boxes with different image sizes, for example, dynamic input shapes. Hence:\n",
    "\n",
    "1. Input dimension with dynamic input shapes needs to be specified before loading text recognition model.\n",
    "2. Dynamic shape is specified by assigning -1 to the input dimension or by setting the upper bound of the input dimension using, for example, `Dimension(1, 512)`.\n",
    "\n",
    ">Note: Since the text recognition model is with dynamic input shape and current release of OpenVINO 2022.2 does not support dynamic shape on iGPU, you cannot directly switch device to iGPU for inference in this case. Otherwise, you may need to resize the input images to this model into a fixed size and then try running the inference on iGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eef6db-ae79-43fc-aade-382fd07c5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime for text recognition.\n",
    "core = Core()\n",
    "\n",
    "# Read the model and corresponding weights from a file.\n",
    "rec_model = core.read_model(model=rec_model_file_path)\n",
    "\n",
    "# Assign dynamic shapes to every input layer on the last dimension.\n",
    "for input_layer in rec_model.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[3] = -1\n",
    "    rec_model.reshape({input_layer: input_shape})\n",
    "\n",
    "rec_compiled_model = core.compile_model(model=rec_model, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output nodes.\n",
    "rec_input_layer = rec_compiled_model.input(0)\n",
    "rec_output_layer = rec_compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd2f20-103b-42dc-b2c5-ca71e9f5dd1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration and Helper functions\n",
    "\n",
    "To structure output, we should first config some parameters (for example, the coordinates of the corners of the screen).\n",
    "\n",
    "Then, use the following helper functions for preprocessing and postprocessing frames:\n",
    "\n",
    "1. Preprocessing the input image: use affine transformations to normalize skewed images.\n",
    "2. Preprocessing for text recognition: resize and normalize detected box images to the same size (for example, `(3, 32, 320)` size for images with Chinese text) for easy batching in inference.\n",
    "3. Postprocessing for structure output: fix some errors in recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006146a-07c4-4b9d-9f3d-cb2f97a71aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "There are four information to config:\n",
    "1. POINTS: the coordinates of the corners of the screen\n",
    "2. DESIGN_SHAPE: the original shape of the screen\n",
    "3. DESIGN_LAYOUT: the elements and their positions in the screen, which is in a standard DESIGN_SHAPE\n",
    "4. RESULT_TEMP: a template of output, which is a dictory. The keys in the `dict` are output features.\n",
    "\n",
    "In the example image, there are 8 features to output.\n",
    "1. Info_Probe: Probe information of power frequency field strength meter. \"探头:---\" means there is no information about Porbe.\n",
    "2. Freq_Set: The work frequency\n",
    "3. Val_Total: The value measured. \"无探头\" means there is no information about Porbe, otherwise, there should be a float value.\n",
    "4. Val_X: Value from x axis.\n",
    "5. Val_Y: Value from y axis.\n",
    "6. Val_Z: Value from z axis.\n",
    "7. Unit\n",
    "8. Field: One of Conventional, electric field, magnetic field. The chinese words \"电场\" means electric field.\n",
    "\n",
    "<img align='center' src= \"https://user-images.githubusercontent.com/83450930/236680146-5751e291-d509-4d71-a2cb-bfbf35609051.jpg\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3cb3af-2ff2-4b26-96d8-71560ae6ea93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The coordinates of the corners of the screen\n",
    "POINTS = [[1121, 56],    # Left top\n",
    "          [3242, 183],   # right top\n",
    "          [3040, 1841],  # right bottom\n",
    "          [1000, 1543]]   # left bottom\n",
    "\n",
    "# The size of the screen\n",
    "DESIGN_SHAPE = (1300, 1000)\n",
    "\n",
    "# Output template\n",
    "RESULT_TEMP = {\"Info_Probe\":\"探头:---\", \n",
    "               \"Freq_Set\":\"\", \n",
    "               \"Val_Total\":\"无探头\", \n",
    "               \"Val_X\":\"\", \n",
    "               \"Val_Y\":\"\", \n",
    "               \"Val_Z\":\"\", \n",
    "               \"Unit\":\"A/m\", \n",
    "               \"Field\":\"常规\"}\n",
    "\n",
    "# features and the layout information\n",
    "DESIGN_LAYOUT = {'Info_Probe':[14, 36, 410, 135],  # feature_name, xmin, ymin, xmax, ymax\n",
    "                 'Freq_Set':[5, 290, 544, 406], \n",
    "                 'Val_Total':[52, 419, 1256, 741], \n",
    "                 'Val_X':[19, 774, 433, 882], \n",
    "                 'Val_Y':[433, 773, 874, 884], \n",
    "                 'Val_Z':[873, 773, 1276, 883], \n",
    "                 'Unit':[1064, 291, 1295, 403], \n",
    "                 'Field':[5, 913, 243, 998]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fc249-ba1c-40b9-889b-4bfd04cc7576",
   "metadata": {},
   "source": [
    "### Preprocessing the input image\n",
    "\n",
    "Use affine transformations to normalize skewed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c381bcb-04ba-49b0-b8ec-1cf90b5d4e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_processing(img, point_list, target_shape):\n",
    "    # affine transformations\n",
    "    # point list is the coordinates of the corners of the screen\n",
    "    # target shape is the design shape\n",
    "    \n",
    "    target_w, target_h = target_shape\n",
    "    pts1 = np.float32(point_list)\n",
    "    pts2 = np.float32([[0, 0],[target_w,0],[target_w, target_h],[0,target_h]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    img2 = cv2.warpPerspective(img, M, (target_w,target_h))\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a7459-5fc8-4254-a32e-a88f701493c6",
   "metadata": {},
   "source": [
    "### Preprocessing Image Functions for Text Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fbc3d-0adc-4360-b1fa-10248ccc4128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess for text recognition.\n",
    "def resize_norm_img(img, max_wh_ratio):\n",
    "    \"\"\"\n",
    "    Resize input image for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img: bounding box image from text detection \n",
    "        max_wh_ratio: value for the resizing for text recognition model\n",
    "    \"\"\"\n",
    "    rec_image_shape = [3, 48, 320]\n",
    "    imgC, imgH, imgW = rec_image_shape\n",
    "    assert imgC == img.shape[2]\n",
    "    character_type = \"ch\"\n",
    "    if character_type == \"ch\":\n",
    "        imgW = int((32 * max_wh_ratio))\n",
    "    h, w = img.shape[:2]\n",
    "    ratio = w / float(h)\n",
    "    if math.ceil(imgH * ratio) > imgW:\n",
    "        resized_w = imgW\n",
    "    else:\n",
    "        resized_w = int(math.ceil(imgH * ratio))\n",
    "    resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "    resized_image = resized_image.astype('float32')\n",
    "    resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "    resized_image -= 0.5\n",
    "    resized_image /= 0.5\n",
    "    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "    padding_im[:, :, 0:resized_w] = resized_image\n",
    "    return padding_im\n",
    "\n",
    "\n",
    "def prep_for_rec(dt_boxes, frame):\n",
    "    \"\"\"\n",
    "    Preprocessing of the detected bounding boxes for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        dt_boxes: detected bounding boxes from text detection \n",
    "        frame: original input frame \n",
    "    \"\"\"\n",
    "    ori_im = frame.copy()\n",
    "    img_crop_list = [] \n",
    "    for bno in range(len(dt_boxes)):\n",
    "        tmp_box = copy.deepcopy(dt_boxes[bno])\n",
    "        img_crop = processing.get_rotate_crop_image(ori_im, tmp_box)\n",
    "        img_crop_list.append(img_crop)\n",
    "        \n",
    "    img_num = len(img_crop_list)\n",
    "    # Calculate the aspect ratio of all text bars.\n",
    "    width_list = []\n",
    "    for img in img_crop_list:\n",
    "        width_list.append(img.shape[1] / float(img.shape[0]))\n",
    "    \n",
    "    # Sorting can speed up the recognition process.\n",
    "    indices = np.argsort(np.array(width_list))\n",
    "    return img_crop_list, img_num, indices\n",
    "\n",
    "\n",
    "def batch_text_box(img_crop_list, img_num, indices, beg_img_no, batch_num):\n",
    "    \"\"\"\n",
    "    Batch for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img_crop_list: processed detected bounding box images \n",
    "        img_num: number of bounding boxes from text detection\n",
    "        indices: sorting for bounding boxes to speed up text recognition\n",
    "        beg_img_no: the beginning number of bounding boxes for each batch of text recognition inference\n",
    "        batch_num: number of images for each batch\n",
    "    \"\"\"\n",
    "    norm_img_batch = []\n",
    "    max_wh_ratio = 0\n",
    "    end_img_no = min(img_num, beg_img_no + batch_num)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        h, w = img_crop_list[indices[ino]].shape[0:2]\n",
    "        wh_ratio = w * 1.0 / h\n",
    "        max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        norm_img = resize_norm_img(img_crop_list[indices[ino]], max_wh_ratio)\n",
    "        norm_img = norm_img[np.newaxis, :]\n",
    "        norm_img_batch.append(norm_img)\n",
    "\n",
    "    norm_img_batch = np.concatenate(norm_img_batch)\n",
    "    norm_img_batch = norm_img_batch.copy()\n",
    "    return norm_img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d9859-17b5-4969-b184-e77b44aeec2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Post-processing\n",
    "\n",
    "Post-processing is a very personal step. The image to be recognized may appear blurry, halo, etc., and there may be some errors in the recognition result, which can be corrected through post-processing.\n",
    "\n",
    "The following code fixes some issues that may exist in the example situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f678b-88cf-45ab-89f3-3524e853afa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_processing(results):\n",
    "    # `LF` can be recognized correctly, but the other may be misidentified\n",
    "    if 'LF' in results['Info_Probe']: \n",
    "        results['Info_Probe'] = \"探头:LF-01\"\n",
    "    \n",
    "    # the target infor is `Frequence`, do not need the suffix `实时值`\n",
    "    # we will simply delete the words `实时值`\n",
    "    results['Freq_Set'] = results['Freq_Set'].split('实时值')[0]\n",
    "    \n",
    "    # the target infor is values, do not need the prefix\n",
    "    results['Val_X'] = results['Val_X'].replace(\"X\",\"\").replace(\":\",\"\") \n",
    "    results['Val_Y'] = results['Val_Y'].replace(\"Y\",\"\").replace(\":\",\"\") \n",
    "    results['Val_Z'] = results['Val_Z'].replace(\"Z\",\"\").replace(\":\",\"\") \n",
    "    \n",
    "    # μ is easy to be recognized as u, and '/' is aesy to be ignored\n",
    "    if 'T' in results['Unit']: \n",
    "        results['Unit'] = \"μT\"\n",
    "    elif 'kV' or 'kv' in results['Unit']:\n",
    "        results['Unit'] = \"kV/m\"\n",
    "    elif 'v' or 'V' in results['Unit']:\n",
    "        results['Unit'] = \"V/m\"\n",
    "    else:\n",
    "        results['Unit'] = \"A/m\"\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2daec-d321-4249-bb17-a52f97cb7577",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb78ff-b376-4777-bce3-b995c079f13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download images\n",
    "IMG_URL = \"https://user-images.githubusercontent.com/83450930/236680146-5751e291-d509-4d71-a2cb-bfbf35609051.jpg\"\n",
    "IMG_FILE_NAME = IMG_URL.split(\"/\")[-1]\n",
    "utils.download_file(IMG_URL, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504e2d5-7fd0-47e1-8500-10f67ebcf96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read images\n",
    "img = cv2.imread(IMG_FILE_NAME)\n",
    "\n",
    "# affine transformations to normalize skewed images\n",
    "img = pre_processing(img, POINTS, DESIGN_SHAPE)\n",
    "\n",
    "# copy the structure output template\n",
    "struct_result = copy.deepcopy(DESIGN_LAYOUT)\n",
    "\n",
    "# structure recognition begins here\n",
    "for key in DESIGN_LAYOUT.keys():\n",
    "    # cut imgs according the layout information\n",
    "    xmin, ymin, xmax, ymax = DESIGN_LAYOUT[key]\n",
    "    cut_img = img[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    h = ymax - ymin # height of cut_img\n",
    "    w = xmax - xmin # width of cut_img\n",
    "    dt_boxes = [np.array([[0,0],[w,0],[w,h],[0,h]],dtype='float32')]\n",
    "    batch_num = 1\n",
    "    \n",
    "    # since the input img is cut, we do not need a detection model to find the position of texts\n",
    "    # Preprocess detection results for recognition.\n",
    "    img_crop_list, img_num, indices = prep_for_rec(dt_boxes, cut_img)\n",
    "\n",
    "    # For storing recognition results, include two parts:\n",
    "    # txts are the recognized text results, scores are the recognition confidence level. \n",
    "    rec_res = [['', 0.0]] * img_num\n",
    "    txts = [] \n",
    "    scores = []\n",
    "\n",
    "    for beg_img_no in range(0, img_num):\n",
    "\n",
    "        # Recognition starts from here.\n",
    "        norm_img_batch = batch_text_box(\n",
    "            img_crop_list, img_num, indices, beg_img_no, batch_num)\n",
    "\n",
    "        # Run inference for text recognition. \n",
    "        rec_results = rec_compiled_model([norm_img_batch])[rec_output_layer]\n",
    "\n",
    "        # Postprocessing recognition results.\n",
    "        postprocess_op = processing.build_post_process(processing.postprocess_params)\n",
    "        rec_result = postprocess_op(rec_results)\n",
    "        for rno in range(len(rec_result)):\n",
    "            rec_res[indices[beg_img_no + rno]] = rec_result[rno]   \n",
    "        if rec_res:\n",
    "            txts = [rec_res[i][0] for i in range(len(rec_res))] \n",
    "            scores = [rec_res[i][1] for i in range(len(rec_res))]\n",
    "    \n",
    "    # record the recognition result\n",
    "    struct_result[key] = txts[0]\n",
    "    \n",
    "# Post-processing, fix some error made in recognition\n",
    "post_processing(struct_result)\n",
    "\n",
    "# Print result\n",
    "print(struct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb52b8-0a6c-4b05-b7ca-d341ffa88bad",
   "metadata": {},
   "source": [
    " ## Try it with your meter photos!\n",
    " \n",
    " For your own photos, you only need to modify the `Configuration` and `post_processing` to run above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
